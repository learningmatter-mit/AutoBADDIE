{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess, ast\n",
    "from munch import Munch\n",
    "from materialbuilder.plotting import CPK_COLORS\n",
    "from materialbuilder import graphbuilder, matbuilder, topologies, transformations\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I wouldnt look at the comments too much yet unless you want to take a huge deepdive into the code\n",
    "### *job name* will be used to create the appropriate directory trees for all training. At the end of this notebook, you can look at AuTopologyPipeline/train/job_name to see the created files, in this case AuTopologyPipeline/train/2311_opls_gLiTfsi_Artur\n",
    "### *template smiles* is how you tell AuTopology what are the species you will be getting parameters for\n",
    "\n",
    "### *node_type* will define how many graph convolutions for atomic environment differentiation (how many covalent nearest neighbors you will consider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details = Munch()\n",
    "job_details.job_name = \"240904_opls_test\"\n",
    "job_details.device = \"cpu\"\n",
    "\n",
    "# glyme4.TFSI.Li\n",
    "job_details.template_smiles = (\n",
    "    \"COCCOCCOCCOCCOC.O=S(=O)([N-]S(=O)(=O)C(F)(F)F)C(F)(F)F.[Li+]\"\n",
    ")\n",
    "job_details.transformations = [\n",
    "    [\"OneHotEncoding\", \"node\", \"atomic_num\", \"one_hot_atomic_num\"],\n",
    "    [\"GraphDistinctNodes\", \"one_hot_atomic_num\", \"one_hot_graph_distinct_r1\", 1],\n",
    "    [\"GraphDistinctNodes\", \"one_hot_atomic_num\", \"one_hot_graph_distinct_r2\", 2],\n",
    "    [\"GraphDistinctNodes\", \"one_hot_atomic_num\", \"one_hot_graph_distinct_r3\", 3],\n",
    "    [\"GraphDistinctNodes\", \"one_hot_atomic_num\", \"one_hot_graph_distinct_r4\", 4],\n",
    "]\n",
    "\n",
    "job_details.base_node_type = \"one_hot_atomic_num\"\n",
    "job_details.node_type = \"one_hot_graph_distinct_r3\"\n",
    "\n",
    "if \"pcff\" in job_details.job_name:\n",
    "    job_details.forcefield_class2 = True\n",
    "else:\n",
    "    job_details.forcefield_class2 = False\n",
    "\n",
    "job_details.terms = [\"bond\", \"angle\", \"dihedral\", \"improper\", \"pair\"]\n",
    "job_details.use_1_4_pairs = True\n",
    "job_details.pair_cutoff = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details.WORKDIR = os.path.join(\n",
    "    os.path.abspath('.'), '../train', job_details.job_name, 'template')\n",
    "if not os.path.exists(job_details.WORKDIR):\n",
    "    os.makedirs(job_details.WORKDIR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Template Dataset (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "template_dataset = graphbuilder.GraphDataset()\n",
    "ensemble = graphbuilder.Ensemble()\n",
    "geometry, rdkit_mol = matbuilder.geometry_from_smiles(\n",
    "    smiles=job_details.template_smiles, dim=2, return_rdkit_mol=True)\n",
    "#add geometry to ensemble.parent_graph and ensemble.graphs if first geom, else add only to ensemble.child_graphs \n",
    "#(do not add children_graphs to ensembel.graphs)\n",
    "ensemble.Add(geometry)\n",
    "#add ensemble to dataset, which will make template_dataset.container_batch have the parent_graph, and self.ensembles \n",
    "#have the ensemble at index ensemble.parent_graph['prop']['ensemble']\n",
    "template_dataset.AddEnsemble(ensemble)\n",
    "#datset.Close() is just a call to dataset.container_batch.Close(), which is just a call to dataset.container_batch._finalize()\n",
    "#this calls graph_base._initialize_data, which primes the dataset.container_batch._data to have 'node' and other keys\n",
    "#then, it calls graph_base._add_data_rom_graph, which will fill the container_batch._data with the info from all the container_batch.graphs\n",
    "#(which is only ever 1 long??)\n",
    "#finally, it calls graph_base._transform_data(join=True), which will cat all the lists in _data together \n",
    "template_dataset.Close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to MOL file using rdkit_mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(job_details.WORKDIR, 'template.mol')\n",
    "file = open(path, 'w')\n",
    "file.write(Chem.MolToMolBlock(rdkit_mol))\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use OpenBabel to convert MOL to PDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_path = os.path.join(job_details.WORKDIR, 'template.mol')\n",
    "pdb_path = os.path.join(job_details.WORKDIR, 'template.pdb')\n",
    "subprocess.call(['obabel', mol_path, '-O', pdb_path]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Template Dataset (Part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## template_dataset = graphbuilder.GraphDataset()\n",
    "# #now we are adding the geometery with dataset.AddGraph(geometry) rather than ensemble.Add(geometry) as before.\n",
    "# #dataset.AddGraph(geometry) will first add a graph._data['prop']['ensemble'] info to specify which ensemble it belongs to\n",
    "# #then the graph is added to the container_batch using container_batch.Add(geometry). this adds the graph to\n",
    "# #container_batch.graphs.append(geometry), which up to this point only contains the single parentgraph, and no others added by ensemble.Add(geometry)\n",
    "# template_dataset.AddGraph(geometry)\n",
    "# #this close will cat the info of these new geometries from dataset.AddGraph with the parent graph info using _transform_data(join=True)\n",
    "# template_dataset.Close()\n",
    "\n",
    "for trans in job_details.transformations:  # \"one_hot_graph_distinct_r2\"\n",
    "    # this will create the different node types based on graph neighborhood. this runs transformation.ApplyTo(template_dataset.container_batch)\n",
    "    template_dataset.AddTransformation(\n",
    "        eval(\"transformations.\" + trans[0])(*tuple(trans[1:])), job_details.device\n",
    "    )\n",
    "if \"base_node_type\" in job_details.keys():\n",
    "    template_dataset.DefineBaseNodeTypes(job_details.base_node_type)\n",
    "if \"node_type\" in job_details.keys():\n",
    "    template_dataset.DefineNodeTypes(job_details.node_type)\n",
    "# these dataset.AddTopology() calls create the topology types based on node type using Graph.AddTopology()\n",
    "# this uses each Topology.apply_to()\n",
    "if \"bond\" in job_details.terms:\n",
    "    template_dataset.AddTopology(topologies.BondTopology(), job_details.device)\n",
    "if \"angle\" in job_details.terms:\n",
    "    template_dataset.AddTopology(topologies.AngleTopology(), job_details.device)\n",
    "if \"dihedral\" in job_details.terms:\n",
    "    template_dataset.AddTopology(topologies.DihedralTopology(), job_details.device)\n",
    "if \"improper\" in job_details.terms:\n",
    "    template_dataset.AddTopology(topologies.ImproperTopology(), job_details.device)\n",
    "if \"pair\" in job_details.terms:\n",
    "    template_dataset.AddTopology(\n",
    "        topologies.PairTopology(\n",
    "            job_details.use_1_4_pairs,\n",
    "            job_details.forcefield_class2,\n",
    "            job_details.pair_cutoff,\n",
    "        ),\n",
    "        job_details.device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(job_details.WORKDIR, 'template.py')\n",
    "torch.save(template_dataset, path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Custom Parameter File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mendeleev import element as Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = template_dataset.container_batch._data['node']['xyz']\n",
    "bonds = template_dataset.container_batch._data['bond']['index']\n",
    "types = template_dataset.container_batch._data['node']['type'].view(-1)\n",
    "z = template_dataset.container_batch._data['node']['atomic_num'].view(-1)\n",
    "z_t = torch.stack([z, types], axis=1).unique(dim=0).tolist()\n",
    "types = types.tolist()\n",
    "z = z.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Atom Types')\n",
    "ax.set_xlim([-5,5])\n",
    "ax.set_ylim([0,100])\n",
    "\n",
    "y = np.linspace(0,100,len(z_t)+2).tolist()[::-1]\n",
    "for n in range(len(z_t)):\n",
    "    element = Element(z_t[n][0]).symbol\n",
    "    ax.scatter(0, y[n+1], s=500, color=CPK_COLORS[z_t[n][0]], alpha=1.0, edgecolors='black', lw=1);\n",
    "    ax.annotate(element+str(z_t[n][1]), (1, y[n+1]-1), color='black', size=15)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.axis('off')\n",
    "plt.savefig(os.path.join(job_details.WORKDIR, 'atom_types_list.png'))  \n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(job_details.template_smiles)\n",
    "\n",
    "x = xyz[:,0].view(-1).tolist()\n",
    "y = xyz[:,1].view(-1).tolist()\n",
    "\n",
    "z_t = torch.stack([template_dataset.container_batch._data['node']['atomic_num'].view(-1), template_dataset.container_batch._data['node']['type'].view(-1)], axis=1)\n",
    "z_t = z_t.tolist()\n",
    "for bond in bonds:\n",
    "    bond = bond.view(-1).tolist()\n",
    "    ax.plot([x[bond[0]], x[bond[1]]], [y[bond[0]], y[bond[1]]], c='black', zorder=0, lw=1)\n",
    "\n",
    "for n in range(len(x)):\n",
    "    ax.scatter(x[n], y[n], s=1000, color=CPK_COLORS[z[n]], alpha=1.0, edgecolors='black', lw=1);\n",
    "    ax.annotate(str(z_t[n][1]), (x[n]-0.25, y[n]-0.025), color='black', size=75)\n",
    "\n",
    "plt.grid(False)\n",
    "plt.axis('off')\n",
    "print(job_details.WORKDIR)\n",
    "plt.savefig(os.path.join(job_details.WORKDIR, 'atom_types.png'))  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For custom_parameter_dict, if one of the parameters is set to None, then the parameter will be learned. If the parameter is given a numeric value, then the value will NOT be learned and will remain as the set value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glyme.Li.TFSI for OPLS parameterization\n",
    "# litfsi comes from this (Doherty 2017): https://pubs.acs.org/doi/10.1021/acs.jctc.7b00520\n",
    "custom_parameter_dict = {\n",
    "    '0': {'types': [0], 'sigma': 2.500, 'epsilon': 0.030, 'charge': None}, #H, CO-()\n",
    "    '1': {'types': [1], 'sigma': 2.500, 'epsilon': 0.030, 'charge': None}, #H, (CCO)\n",
    "    '2': {'types': [2], 'sigma': 2.13, 'epsilon': 0.018, 'charge': None}, #Li \n",
    "    '3': {'types': [3], 'sigma': 3.500, 'epsilon': 0.066, 'charge': None}, #C, CO-()\n",
    "    '4': {'types': [4], 'sigma': 3.500, 'epsilon': 0.066, 'charge': None}, #C, first CO-C*CO(CCO)\n",
    "    '5': {'types': [5], 'sigma': 3.500, 'epsilon': 0.066, 'charge': None}, #C, (CCO)\n",
    "    '6': {'types': [6], 'sigma': 3.500, 'epsilon': 0.066, 'charge': None}, #C, TFSI\n",
    "    '7': {'types': [7], 'sigma': 3.250, 'epsilon': 0.170, 'charge': None}, #N, TFSI\n",
    "    '8': {'types': [8], 'sigma': 2.900, 'epsilon': 0.140, 'charge': None}, #O, CO-(), glyme\n",
    "    '9': {'types': [9], 'sigma': 2.900, 'epsilon': 0.140, 'charge': None}, #O, (CCO), glyme\n",
    "    '10': {'types': [10], 'sigma': 2.960, 'epsilon': 0.210, 'charge': None}, #O=, TFSI\n",
    "    '11': {'types': [11], 'sigma': 2.950, 'epsilon': 0.053, 'charge': None}, #F, TFSI\n",
    "    '12': {'types': [12], 'sigma': 3.550, 'epsilon': 0.250, 'charge': None}, #S, TFSI    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['custom_type'] = list(custom_parameter_dict.keys())\n",
    "for key in ['types', 'sigma', 'epsilon', 'charge']:\n",
    "    df[key] = df['custom_type'].apply(lambda c: custom_parameter_dict[c][key])\n",
    "df = df.set_index('custom_type')\n",
    "path = os.path.join(job_details.WORKDIR, 'template.params')\n",
    "df.to_csv(path)\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Validation\n",
    "\n",
    "### now the files needed to specify the atomic environments have all been created in AuTopologyPipeline/train/job_name/template.\n",
    "### The rest of this code now shows how the parameters set above in custom_parameter_dict are used to always use those paramters to compute forces in Forcefield.pair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_types = len(template_dataset.container_batch.get_data('node', 'type').unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_param = {}\n",
    "for name in ['sigma', 'epsilon', 'charge']:\n",
    "    _param[name] = torch.tensor(list(df[name].fillna(0))).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_map = {}\n",
    "to_type = df['types'].to_dict()\n",
    "for name in _param.keys():\n",
    "    to_custom_type = -1*torch.ones(num_types).to(torch.long)\n",
    "    for custom_type in to_type.keys():\n",
    "        if np.isnan(df[name][custom_type]):\n",
    "            continue\n",
    "        for t in ast.literal_eval(to_type[custom_type]):\n",
    "            to_custom_type[t] = custom_type\n",
    "    type_map[name] = to_custom_type.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['sigma'] = 0.0001*torch.randn(num_types)\n",
    "param['epsilon'] = 0.0001*torch.randn(num_types)\n",
    "param['charge'] = 0.0001*torch.randn(num_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = torch.arange(num_types).to(torch.long).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM = {}\n",
    "for name in param.keys():\n",
    "    custom_types = type_map[name][types.view(-1)]\n",
    "    custom_type_mask = (custom_types<0).to(torch.float)\n",
    "    PARAM[name] = 0.0\n",
    "    PARAM[name] += _param[name][custom_types]*(1-custom_type_mask)\n",
    "    PARAM[name] += param[name][types]*custom_type_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_mask = template_dataset._data['node']['atomic_num']==3\n",
    "print()\n",
    "print(template_dataset._data['node']['one_hot_graph_distinct_r1'][li_mask.view(-1)])\n",
    "# print(template_dataset._data['node']['base_type'].view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40fe4677bea878bbf1c80a80c4be557dae6dc762b83a85ec00a6939071e027e3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
